from transformers import BertTokenizer, BertForMaskedLM, TrainingArguments, Trainer



def train(data_path, model_path, epochs){
    data_path = data_path.rpartition('/')[2]
    model_path = model_path.rpartition('/')[2]


    training_args = TrainingArguments(
        output_dir='out',
        batch_size =16,
        epochs=2
    )

    trainer = Trainer(
        model=model_path ,
        args=args,
        train_dataset=dataset
    )

    trainer.train()

    trainer.save(model_path)
}


def main(data_path: str, model_path: str, epochs: bool = False):
  if formal:
    typer.echo(f"Loading ...")
  else:
    typer.echo(f"It's a better convention to use --epoch. Next time!")
    typer.echo(f"Loading ...")
  
  train(data_path, model_path, epochs)


if __name__ == "__main__":
    typer.run(main)
    
    
    
